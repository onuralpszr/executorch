{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e09712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found library at: /opt/anaconda3/envs/transformers/lib/python3.10/site-packages/torchao/libtorchao_ops_aten.dylib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 03:13:24.937000 50542 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "/opt/anaconda3/envs/transformers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "from executorch.devtools import BundledProgram, generate_etrecord, Inspector\n",
    "from executorch.devtools.bundled_program.config import MethodTestCase, MethodTestSuite\n",
    "from executorch.exir import to_edge_transform_and_lower, EdgeCompileConfig\n",
    "from executorch.exir.program import EdgeProgramManager, to_edge\n",
    "from executorch.extension.pybindings.portable_lib import _load_for_executorch, _load_for_executorch_from_buffer\n",
    "import executorch.extension.pybindings.portable_lib as portable_lib\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import VoxtralEncoder, VoxtralForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import gc\n",
    "import os\n",
    "\n",
    "WORKING_DIR = \"/Users/jackzhxng/Documents/voxtral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93443418",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class VoxtralEncoderForExecuTorch(nn.Module):\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        super().__init__()\n",
    "        self.audio_encoder =  model.audio_tower\n",
    "        self.mm_projector = model.multi_modal_projector\n",
    "        self.intermediate_size = model.config.audio_config.intermediate_size\n",
    "        self.audio_token_id = model.config.audio_token_id\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_features: torch.FloatTensor,\n",
    "    ):\n",
    "        audio_outputs = self.audio_encoder(input_features)\n",
    "        audio_hidden_states = audio_outputs.last_hidden_state\n",
    "        audio_hidden_states = audio_hidden_states.reshape(-1, self.intermediate_size)\n",
    "        audio_embeds = self.mm_projector(audio_hidden_states)\n",
    "\n",
    "        return audio_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33a0fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆ| 2/2 [00:00<00:00, 29.95it/s]\n",
      "Some weights of VoxtralEncoder were not initialized from the model checkpoint at mistralai/Voxtral-Mini-3B-2507 and are newly initialized: ['conv1.bias', 'conv1.weight', 'conv2.bias', 'conv2.weight', 'embed_positions.weight', 'layer_norm.bias', 'layer_norm.weight', 'layers.0.fc1.bias', 'layers.0.fc1.weight', 'layers.0.fc2.bias', 'layers.0.fc2.weight', 'layers.0.final_layer_norm.bias', 'layers.0.final_layer_norm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.out_proj.bias', 'layers.0.self_attn.out_proj.weight', 'layers.0.self_attn.q_proj.bias', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.bias', 'layers.0.self_attn.v_proj.weight', 'layers.0.self_attn_layer_norm.bias', 'layers.0.self_attn_layer_norm.weight', 'layers.1.fc1.bias', 'layers.1.fc1.weight', 'layers.1.fc2.bias', 'layers.1.fc2.weight', 'layers.1.final_layer_norm.bias', 'layers.1.final_layer_norm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.out_proj.bias', 'layers.1.self_attn.out_proj.weight', 'layers.1.self_attn.q_proj.bias', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.bias', 'layers.1.self_attn.v_proj.weight', 'layers.1.self_attn_layer_norm.bias', 'layers.1.self_attn_layer_norm.weight', 'layers.10.fc1.bias', 'layers.10.fc1.weight', 'layers.10.fc2.bias', 'layers.10.fc2.weight', 'layers.10.final_layer_norm.bias', 'layers.10.final_layer_norm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.out_proj.bias', 'layers.10.self_attn.out_proj.weight', 'layers.10.self_attn.q_proj.bias', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.bias', 'layers.10.self_attn.v_proj.weight', 'layers.10.self_attn_layer_norm.bias', 'layers.10.self_attn_layer_norm.weight', 'layers.11.fc1.bias', 'layers.11.fc1.weight', 'layers.11.fc2.bias', 'layers.11.fc2.weight', 'layers.11.final_layer_norm.bias', 'layers.11.final_layer_norm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.out_proj.bias', 'layers.11.self_attn.out_proj.weight', 'layers.11.self_attn.q_proj.bias', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.bias', 'layers.11.self_attn.v_proj.weight', 'layers.11.self_attn_layer_norm.bias', 'layers.11.self_attn_layer_norm.weight', 'layers.12.fc1.bias', 'layers.12.fc1.weight', 'layers.12.fc2.bias', 'layers.12.fc2.weight', 'layers.12.final_layer_norm.bias', 'layers.12.final_layer_norm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.out_proj.bias', 'layers.12.self_attn.out_proj.weight', 'layers.12.self_attn.q_proj.bias', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.bias', 'layers.12.self_attn.v_proj.weight', 'layers.12.self_attn_layer_norm.bias', 'layers.12.self_attn_layer_norm.weight', 'layers.13.fc1.bias', 'layers.13.fc1.weight', 'layers.13.fc2.bias', 'layers.13.fc2.weight', 'layers.13.final_layer_norm.bias', 'layers.13.final_layer_norm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.out_proj.bias', 'layers.13.self_attn.out_proj.weight', 'layers.13.self_attn.q_proj.bias', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.bias', 'layers.13.self_attn.v_proj.weight', 'layers.13.self_attn_layer_norm.bias', 'layers.13.self_attn_layer_norm.weight', 'layers.14.fc1.bias', 'layers.14.fc1.weight', 'layers.14.fc2.bias', 'layers.14.fc2.weight', 'layers.14.final_layer_norm.bias', 'layers.14.final_layer_norm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.out_proj.bias', 'layers.14.self_attn.out_proj.weight', 'layers.14.self_attn.q_proj.bias', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.bias', 'layers.14.self_attn.v_proj.weight', 'layers.14.self_attn_layer_norm.bias', 'layers.14.self_attn_layer_norm.weight', 'layers.15.fc1.bias', 'layers.15.fc1.weight', 'layers.15.fc2.bias', 'layers.15.fc2.weight', 'layers.15.final_layer_norm.bias', 'layers.15.final_layer_norm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.out_proj.bias', 'layers.15.self_attn.out_proj.weight', 'layers.15.self_attn.q_proj.bias', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.bias', 'layers.15.self_attn.v_proj.weight', 'layers.15.self_attn_layer_norm.bias', 'layers.15.self_attn_layer_norm.weight', 'layers.16.fc1.bias', 'layers.16.fc1.weight', 'layers.16.fc2.bias', 'layers.16.fc2.weight', 'layers.16.final_layer_norm.bias', 'layers.16.final_layer_norm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.out_proj.bias', 'layers.16.self_attn.out_proj.weight', 'layers.16.self_attn.q_proj.bias', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.bias', 'layers.16.self_attn.v_proj.weight', 'layers.16.self_attn_layer_norm.bias', 'layers.16.self_attn_layer_norm.weight', 'layers.17.fc1.bias', 'layers.17.fc1.weight', 'layers.17.fc2.bias', 'layers.17.fc2.weight', 'layers.17.final_layer_norm.bias', 'layers.17.final_layer_norm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.out_proj.bias', 'layers.17.self_attn.out_proj.weight', 'layers.17.self_attn.q_proj.bias', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.bias', 'layers.17.self_attn.v_proj.weight', 'layers.17.self_attn_layer_norm.bias', 'layers.17.self_attn_layer_norm.weight', 'layers.18.fc1.bias', 'layers.18.fc1.weight', 'layers.18.fc2.bias', 'layers.18.fc2.weight', 'layers.18.final_layer_norm.bias', 'layers.18.final_layer_norm.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.out_proj.bias', 'layers.18.self_attn.out_proj.weight', 'layers.18.self_attn.q_proj.bias', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.bias', 'layers.18.self_attn.v_proj.weight', 'layers.18.self_attn_layer_norm.bias', 'layers.18.self_attn_layer_norm.weight', 'layers.19.fc1.bias', 'layers.19.fc1.weight', 'layers.19.fc2.bias', 'layers.19.fc2.weight', 'layers.19.final_layer_norm.bias', 'layers.19.final_layer_norm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.out_proj.bias', 'layers.19.self_attn.out_proj.weight', 'layers.19.self_attn.q_proj.bias', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.bias', 'layers.19.self_attn.v_proj.weight', 'layers.19.self_attn_layer_norm.bias', 'layers.19.self_attn_layer_norm.weight', 'layers.2.fc1.bias', 'layers.2.fc1.weight', 'layers.2.fc2.bias', 'layers.2.fc2.weight', 'layers.2.final_layer_norm.bias', 'layers.2.final_layer_norm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.out_proj.bias', 'layers.2.self_attn.out_proj.weight', 'layers.2.self_attn.q_proj.bias', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.bias', 'layers.2.self_attn.v_proj.weight', 'layers.2.self_attn_layer_norm.bias', 'layers.2.self_attn_layer_norm.weight', 'layers.20.fc1.bias', 'layers.20.fc1.weight', 'layers.20.fc2.bias', 'layers.20.fc2.weight', 'layers.20.final_layer_norm.bias', 'layers.20.final_layer_norm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.out_proj.bias', 'layers.20.self_attn.out_proj.weight', 'layers.20.self_attn.q_proj.bias', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.bias', 'layers.20.self_attn.v_proj.weight', 'layers.20.self_attn_layer_norm.bias', 'layers.20.self_attn_layer_norm.weight', 'layers.21.fc1.bias', 'layers.21.fc1.weight', 'layers.21.fc2.bias', 'layers.21.fc2.weight', 'layers.21.final_layer_norm.bias', 'layers.21.final_layer_norm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.out_proj.bias', 'layers.21.self_attn.out_proj.weight', 'layers.21.self_attn.q_proj.bias', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.bias', 'layers.21.self_attn.v_proj.weight', 'layers.21.self_attn_layer_norm.bias', 'layers.21.self_attn_layer_norm.weight', 'layers.22.fc1.bias', 'layers.22.fc1.weight', 'layers.22.fc2.bias', 'layers.22.fc2.weight', 'layers.22.final_layer_norm.bias', 'layers.22.final_layer_norm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.out_proj.bias', 'layers.22.self_attn.out_proj.weight', 'layers.22.self_attn.q_proj.bias', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.bias', 'layers.22.self_attn.v_proj.weight', 'layers.22.self_attn_layer_norm.bias', 'layers.22.self_attn_layer_norm.weight', 'layers.23.fc1.bias', 'layers.23.fc1.weight', 'layers.23.fc2.bias', 'layers.23.fc2.weight', 'layers.23.final_layer_norm.bias', 'layers.23.final_layer_norm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.out_proj.bias', 'layers.23.self_attn.out_proj.weight', 'layers.23.self_attn.q_proj.bias', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.bias', 'layers.23.self_attn.v_proj.weight', 'layers.23.self_attn_layer_norm.bias', 'layers.23.self_attn_layer_norm.weight', 'layers.24.fc1.bias', 'layers.24.fc1.weight', 'layers.24.fc2.bias', 'layers.24.fc2.weight', 'layers.24.final_layer_norm.bias', 'layers.24.final_layer_norm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.out_proj.bias', 'layers.24.self_attn.out_proj.weight', 'layers.24.self_attn.q_proj.bias', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.bias', 'layers.24.self_attn.v_proj.weight', 'layers.24.self_attn_layer_norm.bias', 'layers.24.self_attn_layer_norm.weight', 'layers.25.fc1.bias', 'layers.25.fc1.weight', 'layers.25.fc2.bias', 'layers.25.fc2.weight', 'layers.25.final_layer_norm.bias', 'layers.25.final_layer_norm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.out_proj.bias', 'layers.25.self_attn.out_proj.weight', 'layers.25.self_attn.q_proj.bias', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.bias', 'layers.25.self_attn.v_proj.weight', 'layers.25.self_attn_layer_norm.bias', 'layers.25.self_attn_layer_norm.weight', 'layers.26.fc1.bias', 'layers.26.fc1.weight', 'layers.26.fc2.bias', 'layers.26.fc2.weight', 'layers.26.final_layer_norm.bias', 'layers.26.final_layer_norm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.out_proj.bias', 'layers.26.self_attn.out_proj.weight', 'layers.26.self_attn.q_proj.bias', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.bias', 'layers.26.self_attn.v_proj.weight', 'layers.26.self_attn_layer_norm.bias', 'layers.26.self_attn_layer_norm.weight', 'layers.27.fc1.bias', 'layers.27.fc1.weight', 'layers.27.fc2.bias', 'layers.27.fc2.weight', 'layers.27.final_layer_norm.bias', 'layers.27.final_layer_norm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.out_proj.bias', 'layers.27.self_attn.out_proj.weight', 'layers.27.self_attn.q_proj.bias', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.bias', 'layers.27.self_attn.v_proj.weight', 'layers.27.self_attn_layer_norm.bias', 'layers.27.self_attn_layer_norm.weight', 'layers.28.fc1.bias', 'layers.28.fc1.weight', 'layers.28.fc2.bias', 'layers.28.fc2.weight', 'layers.28.final_layer_norm.bias', 'layers.28.final_layer_norm.weight', 'layers.28.self_attn.k_proj.weight', 'layers.28.self_attn.out_proj.bias', 'layers.28.self_attn.out_proj.weight', 'layers.28.self_attn.q_proj.bias', 'layers.28.self_attn.q_proj.weight', 'layers.28.self_attn.v_proj.bias', 'layers.28.self_attn.v_proj.weight', 'layers.28.self_attn_layer_norm.bias', 'layers.28.self_attn_layer_norm.weight', 'layers.29.fc1.bias', 'layers.29.fc1.weight', 'layers.29.fc2.bias', 'layers.29.fc2.weight', 'layers.29.final_layer_norm.bias', 'layers.29.final_layer_norm.weight', 'layers.29.self_attn.k_proj.weight', 'layers.29.self_attn.out_proj.bias', 'layers.29.self_attn.out_proj.weight', 'layers.29.self_attn.q_proj.bias', 'layers.29.self_attn.q_proj.weight', 'layers.29.self_attn.v_proj.bias', 'layers.29.self_attn.v_proj.weight', 'layers.29.self_attn_layer_norm.bias', 'layers.29.self_attn_layer_norm.weight', 'layers.3.fc1.bias', 'layers.3.fc1.weight', 'layers.3.fc2.bias', 'layers.3.fc2.weight', 'layers.3.final_layer_norm.bias', 'layers.3.final_layer_norm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.out_proj.bias', 'layers.3.self_attn.out_proj.weight', 'layers.3.self_attn.q_proj.bias', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.bias', 'layers.3.self_attn.v_proj.weight', 'layers.3.self_attn_layer_norm.bias', 'layers.3.self_attn_layer_norm.weight', 'layers.30.fc1.bias', 'layers.30.fc1.weight', 'layers.30.fc2.bias', 'layers.30.fc2.weight', 'layers.30.final_layer_norm.bias', 'layers.30.final_layer_norm.weight', 'layers.30.self_attn.k_proj.weight', 'layers.30.self_attn.out_proj.bias', 'layers.30.self_attn.out_proj.weight', 'layers.30.self_attn.q_proj.bias', 'layers.30.self_attn.q_proj.weight', 'layers.30.self_attn.v_proj.bias', 'layers.30.self_attn.v_proj.weight', 'layers.30.self_attn_layer_norm.bias', 'layers.30.self_attn_layer_norm.weight', 'layers.31.fc1.bias', 'layers.31.fc1.weight', 'layers.31.fc2.bias', 'layers.31.fc2.weight', 'layers.31.final_layer_norm.bias', 'layers.31.final_layer_norm.weight', 'layers.31.self_attn.k_proj.weight', 'layers.31.self_attn.out_proj.bias', 'layers.31.self_attn.out_proj.weight', 'layers.31.self_attn.q_proj.bias', 'layers.31.self_attn.q_proj.weight', 'layers.31.self_attn.v_proj.bias', 'layers.31.self_attn.v_proj.weight', 'layers.31.self_attn_layer_norm.bias', 'layers.31.self_attn_layer_norm.weight', 'layers.4.fc1.bias', 'layers.4.fc1.weight', 'layers.4.fc2.bias', 'layers.4.fc2.weight', 'layers.4.final_layer_norm.bias', 'layers.4.final_layer_norm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.out_proj.bias', 'layers.4.self_attn.out_proj.weight', 'layers.4.self_attn.q_proj.bias', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.bias', 'layers.4.self_attn.v_proj.weight', 'layers.4.self_attn_layer_norm.bias', 'layers.4.self_attn_layer_norm.weight', 'layers.5.fc1.bias', 'layers.5.fc1.weight', 'layers.5.fc2.bias', 'layers.5.fc2.weight', 'layers.5.final_layer_norm.bias', 'layers.5.final_layer_norm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.out_proj.bias', 'layers.5.self_attn.out_proj.weight', 'layers.5.self_attn.q_proj.bias', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.bias', 'layers.5.self_attn.v_proj.weight', 'layers.5.self_attn_layer_norm.bias', 'layers.5.self_attn_layer_norm.weight', 'layers.6.fc1.bias', 'layers.6.fc1.weight', 'layers.6.fc2.bias', 'layers.6.fc2.weight', 'layers.6.final_layer_norm.bias', 'layers.6.final_layer_norm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.out_proj.bias', 'layers.6.self_attn.out_proj.weight', 'layers.6.self_attn.q_proj.bias', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.bias', 'layers.6.self_attn.v_proj.weight', 'layers.6.self_attn_layer_norm.bias', 'layers.6.self_attn_layer_norm.weight', 'layers.7.fc1.bias', 'layers.7.fc1.weight', 'layers.7.fc2.bias', 'layers.7.fc2.weight', 'layers.7.final_layer_norm.bias', 'layers.7.final_layer_norm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.out_proj.bias', 'layers.7.self_attn.out_proj.weight', 'layers.7.self_attn.q_proj.bias', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.bias', 'layers.7.self_attn.v_proj.weight', 'layers.7.self_attn_layer_norm.bias', 'layers.7.self_attn_layer_norm.weight', 'layers.8.fc1.bias', 'layers.8.fc1.weight', 'layers.8.fc2.bias', 'layers.8.fc2.weight', 'layers.8.final_layer_norm.bias', 'layers.8.final_layer_norm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.out_proj.bias', 'layers.8.self_attn.out_proj.weight', 'layers.8.self_attn.q_proj.bias', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.bias', 'layers.8.self_attn.v_proj.weight', 'layers.8.self_attn_layer_norm.bias', 'layers.8.self_attn_layer_norm.weight', 'layers.9.fc1.bias', 'layers.9.fc1.weight', 'layers.9.fc2.bias', 'layers.9.fc2.weight', 'layers.9.final_layer_norm.bias', 'layers.9.final_layer_norm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.out_proj.bias', 'layers.9.self_attn.out_proj.weight', 'layers.9.self_attn.q_proj.bias', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.bias', 'layers.9.self_attn.v_proj.weight', 'layers.9.self_attn_layer_norm.bias', 'layers.9.self_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n",
    "processor = AutoProcessor.from_pretrained(repo_id)\n",
    "# model = VoxtralForConditionalGeneration.from_pretrained(repo_id, device_map=device)\n",
    "voxtral_encoder = VoxtralEncoder.from_pretrained(repo_id, device_map=device)\n",
    "# voxtral_encoder = VoxtralEncoderForExecuTorch(model)\n",
    "\n",
    "# Run eager for baseline results\n",
    "# expected_seq_length = model.audio_tower.config.max_source_positions * model.audio_tower.conv1.stride[0] * model.audio_tower.conv2.stride[0]  # From https://github.com/huggingface/transformers/blob/main/src/transformers/models/voxtral/modeling_voxtral.py#L342, should be equal to 3000.\n",
    "# sample_input_features = torch.rand(3, 128, expected_seq_length, dtype=torch.float32)  # Shape of input_features from sample Voxtral audio input from voxtral.md, but with batch size = 1 (representing < 30 seconds of audio). See https://github.com/huggingface/transformers/blob/fbeaf96f9e2291c21277ac658a33ea8752728bf3/src/transformers/models/voxtral/processing_voxtral.py#L91 for more info.\n",
    "# eager_output = model.get_audio_embeds(sample_input_features)\n",
    "sample_input_features = torch.rand(3, 128, 3000, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e61dc22-f38a-486a-9d88-110799c4868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model.language_model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5c557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # {\n",
    "            #     \"type\": \"audio\",\n",
    "            #     \"url\": \"https://huggingface.co/datasets/eustlb/audio-samples/resolve/main/dude_where_is_my_car.wav\",\n",
    "            # },\n",
    "            {\"type\": \"text\", \"text\": \"What can you tell me about this audio?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "inputs = processor.apply_chat_template(conversation)\n",
    "# outputs = model.generate(**inputs, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d125ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_kwargs = {\n",
    "    \"input_features\": sample_input_features,  # (bsz, features, seq_len)\n",
    "}\n",
    "\n",
    "max_audio_len = 150  # In s, should be a multiple of 30.\n",
    "max_seq_len = 2048\n",
    "dynamic_shapes = {\n",
    "    \"input_features\": {\n",
    "        0: torch.export.Dim(\"enc_batch_size_dim\", min=0, max=max_audio_len//30),\n",
    "    },\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    ep = torch.export.export(\n",
    "        voxtral_encoder,\n",
    "        args=(),\n",
    "        kwargs=encoder_input_kwargs,\n",
    "        dynamic_shapes=dynamic_shapes,\n",
    "        strict=True,\n",
    "    )\n",
    "\n",
    "# # Free up some memory?\n",
    "# del model\n",
    "# del voxtral_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242da33a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "et_prog = to_edge_transform_and_lower(\n",
    "    ep,\n",
    "    partitioner=[XnnpackPartitioner()],\n",
    "    compile_config=EdgeCompileConfig(\n",
    "        _check_ir_validity=False,\n",
    "        _skip_dim_order=True,\n",
    "    ),\n",
    "    # constant_methods=metadata,\n",
    "    # transform_passes=[RemovePaddingIdxEmbeddingPass()],\n",
    ").to_executorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99098be9-83b2-4a06-9cc9-40eb564132dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in ep.graph.nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315e126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate etrecord using synthetically created edge program manager which just wraps ExportedProgram.\n",
    "\n",
    "# # Decompose first.\n",
    "# functionalized_ep = ep.run_decompositions({})\n",
    "# all_ops_no_decomp = set()\n",
    "# curr_ops_no_decomp, _ = XnnpackPartitioner().ops_to_not_decompose(ep)\n",
    "# all_ops_no_decomp |= set(curr_ops_no_decomp)\n",
    "# table = _default_decomposition_table()\n",
    "#     for op in all_ops_no_decomp:\n",
    "#         table.pop(op, None)\n",
    "# decomposed_ep = program.run_decompositions(functionalized_ep)\n",
    "# decomposed_ep = remove_unused_parameters_pass(decomposed_ep)\n",
    "\n",
    "\n",
    "# edge_manager_for_etrecord = EdgeProgramManager(ep)\n",
    "edge_manager_for_etrecord = to_edge(ep)\n",
    "inputs = [\n",
    "    (sample_input_features,),\n",
    "]\n",
    "method_test_suites = [\n",
    "    MethodTestSuite(\n",
    "        method_name=\"forward\",\n",
    "        test_cases=[\n",
    "            # MethodTestCase(inputs=input, expected_outputs=eager_output)\n",
    "            MethodTestCase(inputs=input, expected_outputs=voxtral_encoder(sample_input_features))\n",
    "            for input in inputs\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "bundled_program = BundledProgram(et_prog, method_test_suites)\n",
    "etrecord_path = os.path.join(WORKING_DIR, \"etrecord.bin\")\n",
    "generate_etrecord(etrecord_path, edge_manager_for_etrecord, bundled_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035a847-ccd4-48bd-b9ea-08dbed1eb1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(et_prog.buffer) * 1024 / (1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e3f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[program.cpp:135] InternalConsistency verification requested but not available"
     ]
    }
   ],
   "source": [
    "et_mod = _load_for_executorch_from_buffer(\n",
    "    et_prog.buffer,\n",
    "    enable_etdump=True,\n",
    "    debug_buffer_size=len(et_prog.buffer) * 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exported program.\n",
    "ep_output = ep.module().forward(sample_input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da623185",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[buffer_data_sink.cpp:47] Ran out of space to store intermediate outputs.\n",
      "[etdump_flatcc.cpp:578] In function log_evalue(), assert failed (offset.ok()): write_tensor_or_return_error() failed to write tensor to debug buffer"
     ]
    }
   ],
   "source": [
    "# Run executorch program while also generating etdump for debugging.\n",
    "etdump_path = os.path.join(WORKING_DIR, \"etdump.etdp\")\n",
    "debug_buffer_path = os.path.join(WORKING_DIR, \"debug_buffer.bin\")\n",
    "et_output = et_mod.run_method(\"forward\", (sample_input_features,))[0]\n",
    "et_mod.write_etdump_result_to_file(etdump_path, debug_buffer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd439c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_debug_graph(df: pd.DataFrame):\n",
    "    max_gap = max([val for sublist in df['gap'] for val in sublist])\n",
    "\n",
    "    df['aot_intermediate_output'] = df['aot_intermediate_output'].astype(str).str[:24] + '...'\n",
    "    df['runtime_intermediate_output'] = df['runtime_intermediate_output'].astype(str).str[:24] + '...'\n",
    "    def format_scientific(x, digits=3):\n",
    "        return f\"{x:.{digits}e}\"\n",
    "    df['gap'] = df['gap'].apply(lambda lst: [format_scientific(val, 3) for val in lst])\n",
    "    with pd.option_context(\n",
    "        \"display.width\",\n",
    "        1000000,\n",
    "        \"display.max_columns\",\n",
    "        None,\n",
    "        \"display.colheader_justify\",\n",
    "        \"left\",\n",
    "    ):\n",
    "        print(df)\n",
    "\n",
    "    print(\"Max gap:\", max_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector = Inspector(\n",
    "    etdump_path=etdump_path,\n",
    "    etrecord=etrecord_path,\n",
    "    debug_buffer_path=debug_buffer_path,\n",
    ")\n",
    "pd.set_option(\"display.width\", 100000)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df = inspector.calculate_numeric_gap(\"MSE\")\n",
    "print_debug_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(eager_output, ep_output)\n",
    "torch.testing.assert_close(ep_output, et_output)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
